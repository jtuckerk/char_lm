experiment_name: testing_bert_charcloze_to_dense_tok
dataset: 'imdb_tok96_char_encoded384charidx2.pt'
dataset_split: [.5, .1]
input_type: char_masked
output_type: only_masked
token_seq_length: 96
num_masks: 2
hyperparameters:
  num_masks: 2
  input_type: char_masked
  output_type: only_masked
  token_seq_length: 96
  char_vocab_size: [70]
  char_embedding_size: [8]
  conv_activation:
    - 'relu'
  attn_conv.kernel|filter_sizes:
    - 
      - [6, 256]
      - [5, 128]
      - [4, 128]
      - [1, 1  ]
  seg1_type:
    - 'unfold'
  seg1.kernel_size:
    - 18
  seg2.kernel|filter_sizes:
    -
      - [1,256]
      - [1,1024]  
      - [1,1024]  
      - [1,12288]
  token_embedding_size:
    - 768
  switchboard_type:
    - rule_based
  sb.sigmoid:
    - step_with_sigmoid_grad
  mask_chars: True
  batch_size:
    - 64
  learning_rate:
    - .0001
    - .00001
  loss_fn:
    - 'xentropy'
  optimizer:
    - 'adam'  
  epochs:
    - 1
  lr_step_size:
    - 1000
  run_validation:
     - True
  seed:
    - 1
  bert_checkpoint:
    - bert_distil_dense_out
  skip_bert:
    - False
  position_embeddings:
    - True
  manual_attention:
    - False
  space_loss_weight:
    - 0
  freeze_modules:
    - []
    - [bert.transformer.layer.0, bert.transformer.layer.1, bert.transformer.layer.2,
      bert.transformer.layer.3, bert.transformer.layer.4, bert.transformer.layer.5,
      bert.embeddings, vocab_layer_norm, vocab_transform, vocab_projector]   
  model_checkpoint:
    - models/70757d8a7+b8d7b7834
