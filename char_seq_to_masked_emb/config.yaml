experiment_name: testing_bert_tok_to_dense_tok
dataset: 'text8_dist_tok_encoded_train.pt'
dataset_split: [.9, .06]
input_type: token_encoded
token_seq_length: 96
num_masks: 6
hyperparameters:
  input_type: token_encoded
  token_seq_length: 96
  num_masks: 6
  batch_size:
    - 64
  learning_rate:
    - .0001
    - .00001
  loss_fn:
    - 'xentropy'
  optimizer:
    - 'adam'  
  epochs:
    - 1
  lr_step_size:
    - 1000
  run_validation:
     - True
  seed:
    - 0
  bert_checkpoint:
    - bert_distil_uncased
  skip_bert:
    - False
  space_loss_weight:
    - 0
  freeze_modules:  
    - ['bert.embeddings',
       'bert.transformer.layer.1',
       'bert.transformer.layer.2',
       'bert.transformer.layer.3',
       'bert.transformer.layer.4',
       'vocab_transform',
       'vocab_projector',]
