{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tucker/sabbatical/predict_bert_embeddings/char_lm/char_seq_to_masked_emb\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from experiment import Net, InitDataset, CreateModel, LoadCheckpoint, RunOne\n",
    "from yaml import safe_load\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exphash = '561fdb2f9'\n",
    "exp = 'results/%s' % exphash\n",
    "with open(exp) as f:\n",
    "  h = safe_load(f.read())\n",
    "h = h['exp_info']  \n",
    "\n",
    "ds = InitDataset(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:32:25] loading configuration file bert_distil_dense_out/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:32:25] loading weights file bert_distil_dense_out/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:32:27] Loaded model: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "#h['hyperparameters']['manual_attention'] = True\n",
    "model = CreateModel(h['hyperparameters'])\n",
    "LoadCheckpoint(model, 'models/%s'%exphash)\n",
    "\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'imdb_tok96_char_encoded384charidx2.pt',\n",
       " 'dataset_split': [0.5, 0.1],\n",
       " 'experiment_hash': '561fdb2f9',\n",
       " 'experiment_name': 'testing_bert_charcloze_to_dense_tok',\n",
       " 'finished_date': '2020-06-18 12:29:50.640342',\n",
       " 'hyperparameters': {'attn_conv.kernel|filter_sizes': [[6, 256],\n",
       "   [5, 128],\n",
       "   [4, 128],\n",
       "   [1, 1]],\n",
       "  'batch_size': 32,\n",
       "  'bert_checkpoint': 'bert_distil_dense_out',\n",
       "  'char_embedding_size': 8,\n",
       "  'char_vocab_size': 70,\n",
       "  'conv_activation': 'relu',\n",
       "  'epochs': 0,\n",
       "  'freeze_modules': ['bert.transformer.layer.0',\n",
       "   'bert.transformer.layer.1',\n",
       "   'bert.transformer.layer.2',\n",
       "   'bert.transformer.layer.3',\n",
       "   'bert.transformer.layer.4',\n",
       "   'bert.transformer.layer.5',\n",
       "   'bert.embeddings',\n",
       "   'vocab_layer_norm',\n",
       "   'vocab_transform',\n",
       "   'vocab_projector',\n",
       "   'embedding_predict'],\n",
       "  'input_type': 'char_masked',\n",
       "  'learning_rate': 0.0001,\n",
       "  'loss_fn': 'xentropy',\n",
       "  'lr_step_size': 1000,\n",
       "  'manual_attention': False,\n",
       "  'mask_chars': True,\n",
       "  'model_checkpoint': 'models/70757d8a7+81270f4d8',\n",
       "  'num_masks': 1,\n",
       "  'optimizer': 'adam',\n",
       "  'output_type': 'only_masked',\n",
       "  'position_embeddings': True,\n",
       "  'run_validation': True,\n",
       "  'sb.sigmoid': 'step_with_sigmoid_grad',\n",
       "  'seed': 0,\n",
       "  'seg1.kernel_size': 18,\n",
       "  'seg1_type': 'unfold',\n",
       "  'seg2.kernel|filter_sizes': [[1, 256], [1, 1024], [1, 1024], [1, 12288]],\n",
       "  'skip_bert': False,\n",
       "  'space_loss_weight': 0,\n",
       "  'switchboard_type': 'rule_based',\n",
       "  'token_embedding_size': 768,\n",
       "  'token_seq_length': 96},\n",
       " 'input_type': 'char_masked',\n",
       " 'max_mem_alloc': 1167865856,\n",
       " 'num_masks': 1,\n",
       " 'output_type': 'only_masked',\n",
       " 'size_bytes': 362447988,\n",
       " 'size_params': 90611997,\n",
       " 'token_seq_length': 96}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h['dataset_split'][1] = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(ds[1], batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0]['label_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2005])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0]['label_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Torch2Py = lambda x: x.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xent = nn.CrossEntropyLoss()\n",
    "def xentropy_loss_fn(output, labels):\n",
    "  output = output.contiguous()  \n",
    "  return xent(output.view(-1, output.size(-1)), labels.view(-1))\n",
    "\n",
    "def top1_acc_fn(output, labels):\n",
    "  top = output.argmax(-1)\n",
    "  right = top==labels\n",
    "  return right.float()\n",
    "\n",
    "def Validate(val_loader, model):\n",
    "  device = next(model.parameters()).device\n",
    "  cum_acc = 0\n",
    "  cum_xent_loss = 0\n",
    "\n",
    "  rights_ = []\n",
    "  inputs_ =[]\n",
    "  labels_ =[]\n",
    "  maxes_ =[]\n",
    "  for i, data in enumerate(val_loader):\n",
    "    data = {k: d.to(device) for k,d in data.items()}\n",
    "    inputs = data['input_ids']\n",
    "    labels = data['label_ids']\n",
    "    inputs_ += Torch2Py(inputs)\n",
    "    labels_ += Torch2Py(labels)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "      model.eval()\n",
    "      outputs = model(inputs)\n",
    "      maxes_ += Torch2Py(outputs.argmax(-1))\n",
    "      xent_loss = xentropy_loss_fn(outputs, labels)\n",
    "      cum_xent_loss += xent_loss      \n",
    "\n",
    "      top1_acc = top1_acc_fn(outputs, labels)\n",
    "      rights_ += Torch2Py(top1_acc)\n",
    "      cum_acc += top1_acc.mean()\n",
    "      break\n",
    "\n",
    "  steps = i+1\n",
    "  print(cum_acc/steps, cum_xent_loss.detach()/steps)\n",
    "  return rights_, inputs_, labels_, maxes_,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4922, device='cuda:0') tensor(3.9828, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "right, inputs, labels, maxes = Validate(val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32, 46, 55, 62, 33,  1, 58, 57,  1, 44, 63,  1, 63, 51, 48],\n",
       "       [32, 46, 55, 62, 33,  1, 62, 58,  1, 51, 52, 62,  1, 51, 48],\n",
       "       [32, 46, 55, 62, 33,  1, 44, 55, 52, 48, 57, 62,  1, 59, 65],\n",
       "       [32, 46, 55, 62, 33,  1, 62, 51, 48,  1, 52, 62,  1, 62, 48],\n",
       "       [32, 46, 55, 62, 33,  1, 51, 48, 55, 59,  1, 51, 48, 61,  1],\n",
       "       [32, 46, 55, 62, 33,  1, 11, 11, 11, 11,  1, 63, 51, 52, 62],\n",
       "       [32, 46, 55, 62, 33,  1, 59, 61, 52, 62, 58, 57, 48, 61, 62],\n",
       "       [32, 46, 55, 62, 33,  1, 63, 51, 48,  1, 59, 61, 48, 56, 52],\n",
       "       [32, 46, 55, 62, 33,  1, 63, 51, 48, 52, 61,  1, 58, 59, 52],\n",
       "       [32, 46, 55, 62, 33,  1, 58, 49,  1, 63, 51, 48,  1, 57, 48]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(inputs)[:10, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 64, 64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(right), len(inputs), len(labels), len(maxes),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 768])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedded_chars[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_to_idx_map = torch.load('../data/char_to_idx_map2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_to_char_map= {v:k for k,v in char_to_idx_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_to_tok_map = {}\n",
    "with open('../data/bert-base-uncased-vocab.txt') as f:\n",
    "  for i, l in enumerate(f.readlines()):\n",
    "    idx_to_tok_map[i] = l.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetChars(encoded):\n",
    "  s = \"\"\n",
    "  for c in encoded:\n",
    "    s+= idx_to_char_map[c]\n",
    "  return s.replace('\\x00','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetToks(encoded):\n",
    "  s = \"\"\n",
    "  for c in encoded:\n",
    "    s+= \" \" + idx_to_tok_map[c]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32, 46, 55, 62, 33,  1, 58, 57,  1, 44, 63,  1, 63, 51, 48,  1, 62, 44,\n",
       "         56, 48,  1, 63, 52, 56, 48, 15,  1, 63, 51, 48, 61, 48,  1, 47, 58, 48,\n",
       "         62, 57,  8, 63,  1, 62, 48, 48, 56,  1, 63, 58,  1, 45, 48,  1, 44, 57,\n",
       "         68,  1, 46, 58, 57, 57, 48, 46, 63, 52, 58, 57,  1, 45, 48, 63, 66, 48,\n",
       "         48, 57,  1, 63, 51, 48,  1, 63, 66, 58,  1,  9, 52,  1, 56, 52, 62, 62,\n",
       "          1, 63, 51, 48,  1, 32, 56, 33,  1, 48, 59, 52, 62, 58, 47, 48, 13,  1,\n",
       "         62, 58,  1, 52,  1, 47, 58, 57,  8, 63,  1, 54, 57, 58, 66,  1, 52, 49,\n",
       "          1, 63, 51, 48, 68,  1, 51, 44, 65, 48,  1, 56, 48, 57, 63, 52, 58, 57,\n",
       "         48, 47,  1, 63, 51, 48,  1, 46, 58, 57, 57, 48, 46, 63, 52, 58, 57, 13,\n",
       "          1, 45, 64, 63,  1, 49, 61, 58, 56,  1, 63, 51, 48,  1, 36, 57, 47,  1,\n",
       "         48, 59, 52, 62, 58, 47, 48,  1, 63, 58,  1, 63, 51, 48,  1, 40, 63, 51,\n",
       "         13,  1, 63, 51, 48, 61, 48,  1, 52, 62,  1, 57, 58, 63, 51, 52, 57, 50,\n",
       "          1, 56, 48, 57, 63, 52, 58, 57, 52, 57, 50,  1, 63, 51, 48,  1, 46, 58,\n",
       "         57, 57, 48, 46, 63, 52, 58, 57,  1, 45, 48, 63, 66, 48, 48, 57,  1, 63,\n",
       "         51, 48,  1, 36,  1,  3, 62, 63, 58, 61, 68,  3, 10,  1, 63, 51, 52, 62,\n",
       "          1, 62, 48, 61, 52, 48, 62,  1, 47, 48, 62, 48, 61, 65, 48, 62,  1, 63,\n",
       "         58,  1, 45, 48, 46, 58, 56, 48,  1, 44,  1, 61, 48, 50, 64, 55, 44, 61,\n",
       "          1, 57, 48, 67, 63,  1, 62, 48, 44, 62, 58, 57,  1, 58, 61,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(inputs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f83d10a26e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_start_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sabbatical/predict_bert_embeddings/char_lm/char_seq_to_masked_emb/experiment.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    571\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0membout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0mchar_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mchar_block_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "model.char_embedder.tokens_start_attn(torch.tensor(inputs).cuda()[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chrs = []\n",
    "for i in inputs:\n",
    "  chrs.append(GetChars(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[cls] on at the same time. there doesn\\'t seem to be any connection between the two (i miss the [m] episode, so i don\\'t know if they have mentioned the connection, but from the 2nd episode to the 6th, there is nothing mentioning the connection between the 2 \"story\") this series deserves to become a regular next season or_______________________________________________________________',\n",
       " \"[cls] so his heroic effort to win big for the south never really became my own goal. i couldn't find my interest in his skimpy story if i had a civil war ree-[m]acter sitting next to me. scene after scene of audie trying to pull it together, and it can only end one way. ...in his triumph over his_______________________________________________________________________________________\",\n",
       " '[cls] aliens pvt hudson & the unwilling tech guy (\"game over man\", vs. \"i\\'m not going out there\") the black hole cowardly dude gets what\\'s coming after stealing the escape pod leviathan: sunken cargo ship with dangerous cargo symbolical breaking to helpless support ship and lots, [m] more. i\\'ll stop here, because my bac is approaching critical. favorite bits:_______________________']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "attn = (model.char_embedder.char_block_attn[:n]>0).float()\n",
    "attn = model.char_embedder.sb_module.switch_input[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rb = (torch.tensor(inputs)<=32)[:n].float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-131-96c00cc89c27>:1: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "spaces = torch.where(inputs==1) # TODO fragile. Alls non alphanumerical characters in char2idxmap2\n",
    "act = torch.zeros_like(inputs)\n",
    "\n",
    "# TODO clean and include other special chars.\n",
    "act[spaces[0], torch.clamp(spaces[1]+1, max=act.shape[-1]-1)] = 1\n",
    "special_chars = torch.where((inputs<=32) & (inputs!=1))\n",
    "act[special_chars] = 1\n",
    "act[torch.where(act==0)]=0\n",
    "act[:, 0] = 1\n",
    "rb = act.clone().cpu()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tucker/Downloads/strategy_heatmap.html'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.figure_factory as ff\n",
    "hm = ff.create_annotated_heatmap(\n",
    "  z=attn.detach().cpu().numpy(), annotation_text=chrs[:n],\n",
    "  text=attn.detach().cpu().numpy(), hoverinfo='text')\n",
    "\n",
    "plotly.offline.plot(hm, filename='/home/tucker/Downloads/strategy_heatmap.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tucker/Downloads/strategy_heatmap.html'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.figure_factory as ff\n",
    "hm = ff.create_annotated_heatmap(\n",
    "  z=rb.numpy(), annotation_text=chrs[:n],\n",
    "  text=attn.detach().cpu().numpy(), hoverinfo='text')\n",
    "\n",
    "plotly.offline.plot(hm, filename='/home/tucker/Downloads/strategy_heatmap.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 0, 0,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = (rb-attn.cpu()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tucker/Downloads/strategy_heatmap.html'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.figure_factory as ff\n",
    "hm = ff.create_annotated_heatmap(\n",
    "  z=acc.cpu(), annotation_text=chrs[:n],\n",
    "  text=attn.detach().cpu().numpy(), hoverinfo='text')\n",
    "\n",
    "plotly.offline.plot(hm, filename='/home/tucker/Downloads/strategy_heatmap.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = ds[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inps = d['input_ids'].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  7, 17, 22, 78])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(torch.randperm(100)[:5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[cls] involves secondary institutions -- the police, psychiatrists, or, quien sabe?, the national security [m]. in the case of marie/audrey[m]robbie/etc., all three systems failed for a long time, simply because it seemed so hard to believe that a woman would kill her husband and try to poison her own child [m] insurance that was no more than modest. and[m]_________________________'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetChars(Torch2Py(inps)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [CLS] agency / for ,'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(d['label_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outs = model(d['input_ids'].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [CLS] guard for for ,'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(outs.argmax(-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [PAD] [unused22] [unused31] [unused68] [unused78]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(d['mask_idxs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 46, 55, 62, 33,  1, 52, 57, 65, 58, 55, 65, 48, 62,  1, 62, 48, 46,\n",
       "        58, 57, 47, 44, 61, 68,  1, 52, 57, 62, 63, 52, 63, 64, 63, 52, 58, 57,\n",
       "        62,  1, 14, 14,  1, 63, 51, 48,  1, 59, 58, 55, 52, 46, 48, 13,  1, 59,\n",
       "        62, 68, 46, 51, 52, 44, 63, 61, 52, 62, 63, 62, 13,  1, 58, 61, 13,  1,\n",
       "        60, 64, 52, 48, 57,  1, 62, 44, 45, 48, 22, 13,  1, 63, 51, 48,  1, 57,\n",
       "        44, 63, 52, 58, 57, 44, 55,  1, 62, 48, 46, 64, 61, 52, 63, 68,  1, 32,\n",
       "        56, 33, 15,  1, 52, 57,  1, 63, 51, 48,  1, 46, 44, 62, 48,  1, 58, 49,\n",
       "         1, 56, 44, 61, 52, 48, 16, 44, 64, 47, 61, 48, 68, 32, 56, 33, 61, 58,\n",
       "        45, 45, 52, 48, 16, 48, 63, 46, 15, 13,  1, 44, 55, 55,  1, 63, 51, 61,\n",
       "        48, 48,  1, 62, 68, 62, 63, 48, 56, 62,  1, 49, 44, 52, 55, 48, 47,  1,\n",
       "        49, 58, 61,  1, 44,  1, 55, 58, 57, 50,  1, 63, 52, 56, 48, 13,  1, 62,\n",
       "        52, 56, 59, 55, 68,  1, 45, 48, 46, 44, 64, 62, 48,  1, 52, 63,  1, 62,\n",
       "        48, 48, 56, 48, 47,  1, 62, 58,  1, 51, 44, 61, 47,  1, 63, 58,  1, 45,\n",
       "        48, 55, 52, 48, 65, 48,  1, 63, 51, 44, 63,  1, 44,  1, 66, 58, 56, 44,\n",
       "        57,  1, 66, 58, 64, 55, 47,  1, 54, 52, 55, 55,  1, 51, 48, 61,  1, 51,\n",
       "        64, 62, 45, 44, 57, 47,  1, 44, 57, 47,  1, 63, 61, 68,  1, 63, 58,  1,\n",
       "        59, 58, 52, 62, 58, 57,  1, 51, 48, 61,  1, 58, 66, 57,  1, 46, 51, 52,\n",
       "        55, 47,  1, 32, 56, 33,  1, 52, 57, 62, 64, 61, 44, 57, 46, 48,  1, 63,\n",
       "        51, 44, 63,  1, 66, 44, 62,  1, 57, 58,  1, 56, 58, 61, 48,  1, 63, 51,\n",
       "        44, 57,  1, 56, 58, 47, 48, 62, 63, 15,  1, 44, 57, 47, 32, 56, 33,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['input_ids'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [CLS] guard for for ,'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(model(inps.cuda()).argmax(-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  a = ((model.embedded_chars[-1, 0:8, :].unsqueeze(1)-model.bert.distilbert.embeddings.word_embeddings.weight.data)**2).mean(-1).argmin(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedded_chars[-1, 0:20, :].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  a = (model.embedded_chars[-1, 0:20, :].squeeze(1)@model.bert.distilbert.embeddings.word_embeddings.weight.data.T).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cls] rat-pelts, is [m]. he scarcely looks human, [m] shadow-image of the successful doctor, yukio. [m]tekichi represents the oppression of japan's violent, disordered-past from the eras of the shogun and the samurai[m] yukio represents the emergence of a modern japan, with his work as a doctor and his bourgeois life with his new-wife[m] rin. he to[m] helping cure the______________\n"
     ]
    }
   ],
   "source": [
    "print(GetChars(inputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[cls] above. how original!). and pacino is [m] by very pretty (and [m]) girls who are in love [m] him. how pathetic. if you really like the \"real[m] al pacino, don\\'t even think to watch this film. you might be pushed to think that extraterrestrials [m] replaced him with a bad copy. al pacino is one of [m] few__________________________________________________________________________'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetChars(Torch2Py(ds[0][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ) \" 7 ) is unknown . he hardly looks human \" ) shadow 15 of the young doctor ) ##yuki ) he represents the oppression of japan . violent ) disorder 15 from the birth of the earth and the samurai ##yuki represents the emergence of a modern japan , with his work as a doctor and his bourgeois life with his friend / ##s . he is help cure the [ [ [ [ [ [ , , , ... ... ... ... ... ... ##oh ##oh ##oh ##oh ##oh ##oh ##oh ##oh ##o\n"
     ]
    }
   ],
   "source": [
    "print(GetToks(maxes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[cls] [m](it remin',\n",
       " [32, 46, 55, 62, 33, 1, 32, 56, 33, 9, 52, 63, 1, 61, 48, 56, 52, 57])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 0\n",
    "inp = inputs[4][j:j+18]\n",
    "GetChars(inp), inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = torch.tensor([[50, 64, 52, 63, 44, 61,  1, 35, 41, 40, 35,  1, 58, 45, 62, 46, 48, 57]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = model.char_embedder.tokens_to_emb(inp.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#next(model.char_embedder.tokens_to_emb.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0719, -0.1866,  0.1774,  0.5457], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = (out@model.bert.distilbert.embeddings.word_embeddings.weight.data.T).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0209, -0.0467, -0.0392, -0.0345,  0.0150, -0.0398, -0.0696, -0.0479,\n",
       "        -0.0290, -0.0037], device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.distilbert.embeddings.word_embeddings.weight.data[2858][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2858], device='cuda:0', grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = ((out-model.bert.distilbert.embeddings.word_embeddings.weight.data)**2).mean(-1).argmin(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "w = '[cls] guitars [m] some ot [m]'\n",
    "thing = []\n",
    "for c in w :\n",
    "  thing.append(char_to_idx_map[c])\n",
    "print(len(thing))\n",
    "inp = torch.tensor(thing).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:44:05] Loaded model: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "LoadCheckpoint(model.char_embedder, '../single_emb_pred/models/c26917206')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = model(inp.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = model.embedded_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = model.char_embedder(inp.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 768])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5e-05"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".005**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5000e-05, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.MSELoss(reduction='none')(torch.tensor([[.005, 0.0, 0.0]]), torch.tensor([[0.0, 0.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = ((out[0, :4].unsqueeze(1)-model.bert.distilbert.embeddings.word_embeddings.weight.data)**2).mean(-1).argmin(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = (out@model.bert.distilbert.embeddings.word_embeddings.weight.data.T).argmax(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [CLS] guitars [MASK] some'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' [CLS] made of stories a where every story is great ; that'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [CLS] retired / it doesn in matter if\n"
     ]
    }
   ],
   "source": [
    "print(GetToks(Torch2Py(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " comedy at at at at at now at at at at at at at now at at comedy at at\n"
     ]
    }
   ],
   "source": [
    "print(GetToks(Torch2Py(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-05c3b8230e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "embeddings = ds.data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c28bfc7a5958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings.norm(2,-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_noise = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = torch.normal(mean=embedding_noise, std=1, size=embeddings.size()).cuda()\n",
    "norm_factor = torch.norm(r, 2, -1).mean()/embedding_noise\n",
    "normalized_noise = r/norm_factor\n",
    "new = embeddings + normalized_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.1433e-05, device='cuda:0')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((new-embeddings)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0072, device='cuda:0')"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt((new-embeddings)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = new.norm(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MakeHist(h, name=None):\n",
    "  y = h[0]\n",
    "  x= h[1]\n",
    "  return {\n",
    "    'name': name,\n",
    "    'type': 'bar',                                                                                                                   'x': x,\n",
    "          'marker': dict(opacity=.7),\n",
    "   'y': y\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dc966d68458f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMakeHist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'toks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'barmode'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'overlay'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "dat = MakeHist(np.histogram(n, bins=30), name='toks')\n",
    "\n",
    "fig = fig = go.Figure(data=[dat], layout={'barmode':'overlay'})\n",
    "plotly.offline.plot(fig, filename='/home/tucker/Downloads/imdb_size_distci.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from experiment import SwitchboardAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from experiment import step_with_sigmoid_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_with_sigmoid_grad(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sba = SwitchboardAttention(6, 'step_with_sigmoid_grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sba(torch.tensor([[1,1,1,1,1]]).float()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attn = torch.nn.Sigmoid()(model.char_embedder.char_block_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tucker/Downloads/strategy_heatmap.html'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm = go.Heatmap(z=attn[:100].detach().cpu().numpy())\n",
    "fig = go.Figure(data=[hm])\n",
    "plotly.offline.plot(fig, filename='/home/tucker/Downloads/strategy_heatmap.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb = model.char_embedder.switchboard[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tucker/Downloads/strategy_heatmap.html'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm = go.Heatmap(z=sb.detach().cpu().numpy())\n",
    "fig = go.Figure(data=[hm])\n",
    "plotly.offline.plot(fig, filename='/home/tucker/Downloads/strategy_heatmap.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 384])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = CharClozeToDenseTokEmbDataset('imdb_tok96_char_encoded384.pt', num_masks=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_to_idx_map = torch.load('../data/char_to_idx_map.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_to_char_map= {v:k for k,v in char_to_idx_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_to_tok_map = {}\n",
    "with open('../data/bert-base-uncased-vocab.txt') as f:\n",
    "  for i, l in enumerate(f.readlines()):\n",
    "    idx_to_tok_map[i] = l.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetChars(encoded):\n",
    "  s = \"\"\n",
    "  for c in encoded:\n",
    "    s+= idx_to_char_map[c]\n",
    "  return s.replace('\\x00','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetToks(encoded):\n",
    "  s = \"\"\n",
    "  for i, c in enumerate(encoded):\n",
    "    s+= \" %d.\"%i + idx_to_tok_map[c]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7f974ae4cd4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetChars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorch2Py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetChars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorch2Py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetToks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorch2Py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "print(GetChars(Torch2Py(ds[0]['input_ids'])))\n",
    "print(GetChars(Torch2Py(ds[0]['original'])))\n",
    "print(GetToks(Torch2Py(ds[0]['tokens'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(76)\n",
      "tensor([[ 72,  76],\n",
      "        [ 92,  95],\n",
      "        [ 96, 101],\n",
      "        [114, 115],\n",
      "        [116, 122],\n",
      "        [122, 123],\n",
      "        [126, 130],\n",
      "        [137, 138],\n",
      "        [139, 142],\n",
      "        [145, 150],\n",
      "        [162, 163],\n",
      "        [168, 171],\n",
      "        [192, 196],\n",
      "        [209, 213],\n",
      "        [220, 221],\n",
      "        [227, 233],\n",
      "        [234, 241],\n",
      "        [252, 253],\n",
      "        [253, 254],\n",
      "        [269, 272]], dtype=torch.int16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([34, 42, 51, 58, 36,  1, 48,  1, 40, 43, 52, 48, 59, 13,  1, 59, 47, 44,\n",
       "          1, 46, 57, 44, 40, 59,  1, 52, 40, 49, 54, 57, 48, 59, 64,  1, 54, 45,\n",
       "          1, 45, 48, 51, 52, 58,  1, 57, 44, 51, 44, 40, 58, 44, 43,  1, 41, 44,\n",
       "         45, 54, 57, 44,  1, 58, 40, 64,  1, 18, 26, 20, 20,  1, 40, 57, 44,  1,\n",
       "         34, 38, 36,  1, 53, 54, 59,  1, 45, 54, 57,  1, 52, 44, 15,  1, 54, 45,\n",
       "          1, 34, 38, 36,  1, 34, 38, 36,  1, 54, 57,  1, 58, 54,  1,  3, 52, 40,\n",
       "         49, 54, 57, 34, 38, 36,  1, 34, 38, 36, 34, 38, 36,  1, 48,  1, 34, 38,\n",
       "         36,  1, 61, 48, 44, 62, 44, 43, 34, 38, 36,  1, 34, 38, 36,  1, 48,  1,\n",
       "         34, 38, 36,  1,  9, 59, 47, 44,  1, 42, 57, 54, 62, 43, 10, 34, 38, 36,\n",
       "          1, 40, 53, 43,  1, 34, 38, 36,  1, 62, 44, 57, 44,  1, 61, 44, 57, 64,\n",
       "          1, 46, 54, 54, 43,  1,  9, 59, 47, 44,  1, 34, 38, 36,  1, 42, 54, 52,\n",
       "         52, 40, 53, 43,  1, 40, 53, 43,  1, 34, 38, 36,  1, 51, 48, 46, 47, 59,\n",
       "         58, 34, 38, 36,  1, 59, 47, 40, 59,  1, 34, 38, 36,  1, 34, 38, 36,  1,\n",
       "         42, 48, 57, 42, 40,  1, 18, 26, 20, 18, 34, 38, 36, 34, 38, 36,  1, 58,\n",
       "         54,  1, 48,  1, 62, 40, 58,  1, 40, 55, 55, 57, 44, 34, 38, 36, 58, 48,\n",
       "         61, 44,  1, 40, 41, 54, 60, 59,  1, 59, 47, 48, 58,  1, 54, 53, 44, 13,\n",
       "          1, 40, 53, 43,  1, 47, 60, 52, 54, 57,  1, 48, 58,  1, 54, 45, 59, 44,\n",
       "         53,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0, 34, 38, 36,  0])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200091, 96, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data['token_start_offsets'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original = ds.data['chars_encoded'][3]\n",
    "offsets = ds.data['token_start_offsets'][3]\n",
    "tokens = ds.data['token_ids'][3]\n",
    "num_toks = (tokens!=0).sum()\n",
    "rand_toks = torch.randperm(num_toks)\n",
    "offsets = offsets[rand_toks[:ds.num_masks]]\n",
    "sorted_offsets = torch.sort(offsets, 0)[0]\n",
    "segments_to_keep = torch.cat([ds.beg, sorted_offsets.view(-1), ds.end], 0).view(-1,2)\n",
    "\n",
    "slices = []\n",
    "char_mask = torch.tensor([34,38,36]).char().to('cuda')\n",
    "for start, end in segments_to_keep:\n",
    "  char_slice = original[start:end]\n",
    "  slices.append(char_slice)\n",
    "  slices.append(char_mask)\n",
    "\n",
    "masked_seq = torch.cat(slices, 0)\n",
    "pad_size = max(0, len(original)-len(masked_seq))\n",
    "pad = torch.tensor([0]).expand(pad_size).char().to('cuda')\n",
    "masked_seq = torch.cat([masked_seq, pad], 0)[:len(original)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'randperm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-322-7fb3e006f95a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_start_offsets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'randperm'"
     ]
    }
   ],
   "source": [
    "original ds.data['token_start_offsets'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[.1,.2,.3,.4,.5]])\n",
    "b = torch.tensor([[.5,.4,.3,.2,.1]])\n",
    "c = torch.tensor([[.1,.1,.1,.1,.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nll(teacher, student):\n",
    "  return -1*(teacher*torch.log(student)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2929)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StepWithSigmoidGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return (x > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self,grad_output):\n",
    "        sig = torch.nn.Sigmoid()(self.x)\n",
    "        return sig*(1-sig)\n",
    "\n",
    "class StepWithHardGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self,x):\n",
    "        self.high = (x>-.5) & (x<.5)\n",
    "        return (x > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self,grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        print(self.high)\n",
    "        grad_input[self.high] = 1\n",
    "        grad_input[~self.high] = 0\n",
    "        return grad_input\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = StepWithHardGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in torch.arange(-2,2,.1):\n",
    "  t = i.clone().detach()\n",
    "  t.requires_grad =True\n",
    "  v = f.apply(t)\n",
    "  v.backward()\n",
    "  vals.append((t.item(), v.item(), t.grad.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-2.0, 0.0, 0.0),\n",
       " (-1.899999976158142, 0.0, 0.0),\n",
       " (-1.7999999523162842, 0.0, 0.0),\n",
       " (-1.7000000476837158, 0.0, 0.0),\n",
       " (-1.600000023841858, 0.0, 0.0),\n",
       " (-1.5, 0.0, 0.0),\n",
       " (-1.399999976158142, 0.0, 0.0),\n",
       " (-1.2999999523162842, 0.0, 0.0),\n",
       " (-1.2000000476837158, 0.0, 0.0),\n",
       " (-1.100000023841858, 0.0, 0.0),\n",
       " (-1.0, 0.0, 0.0),\n",
       " (-0.8999999761581421, 0.0, 0.0),\n",
       " (-0.800000011920929, 0.0, 0.0),\n",
       " (-0.699999988079071, 0.0, 0.0),\n",
       " (-0.6000000238418579, 0.0, 0.0),\n",
       " (-0.5, 0.0, 0.0),\n",
       " (-0.4000000059604645, 0.0, 1.0),\n",
       " (-0.30000001192092896, 0.0, 1.0),\n",
       " (-0.20000000298023224, 0.0, 1.0),\n",
       " (-0.10000000149011612, 0.0, 1.0),\n",
       " (0.0, 0.0, 1.0),\n",
       " (0.10000000149011612, 1.0, 1.0),\n",
       " (0.20000000298023224, 1.0, 1.0),\n",
       " (0.30000001192092896, 1.0, 1.0),\n",
       " (0.4000000059604645, 1.0, 1.0),\n",
       " (0.5, 1.0, 0.0),\n",
       " (0.6000000238418579, 1.0, 0.0),\n",
       " (0.699999988079071, 1.0, 0.0),\n",
       " (0.800000011920929, 1.0, 0.0),\n",
       " (0.8999999761581421, 1.0, 0.0),\n",
       " (1.0, 1.0, 0.0),\n",
       " (1.100000023841858, 1.0, 0.0),\n",
       " (1.2000000476837158, 1.0, 0.0),\n",
       " (1.2999999523162842, 1.0, 0.0),\n",
       " (1.399999976158142, 1.0, 0.0),\n",
       " (1.5, 1.0, 0.0),\n",
       " (1.600000023841858, 1.0, 0.0),\n",
       " (1.7000000476837158, 1.0, 0.0),\n",
       " (1.7999999523162842, 1.0, 0.0),\n",
       " (1.899999976158142, 1.0, 0.0)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t,v,g = zip(*vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat1 = {'type':'scatter', 'x':t, 'y':v}\n",
    "dat2 = {'type':'scatter', 'x':t, 'y':g}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tucker/Downloads/imdb_size_distci.html'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = fig = go.Figure(data=[dat1,dat2])\n",
    "plotly.offline.plot(fig, filename='/home/tucker/Downloads/imdb_size_distci.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotly rescales colors in an incovenient way - manually calculating colors.\n",
    "class rgb():\n",
    "  def __init__(self, r, g, b):\n",
    "    self.r = int(round(r))\n",
    "    self.g = int(round(g))\n",
    "    self.b = int(round(b))\n",
    "    \n",
    "  def __str__(self):\n",
    "    return \"rgb(%d, %d, %d)\" % (self.r, self.g, self.b)\n",
    "  \n",
    "  def interpolate(self, other_color, weight):\n",
    "    # returns a weighted average of this color and other color where the \n",
    "    # weight is a value between 0 and 1. 0 meaning 0% of the other color.\n",
    "    assert weight <=1 and weight >= 0\n",
    "    \n",
    "    def linear_interpolate(c1, c2):\n",
    "      return (c2 - c1) * weight + c1\n",
    "    \n",
    "    r = linear_interpolate(self.r, other_color.r)\n",
    "    g = linear_interpolate(self.g, other_color.g)\n",
    "    b = linear_interpolate(self.b, other_color.b)\n",
    "    \n",
    "    return rgb(r, g, b)\n",
    "\n",
    "\n",
    "color0 = rgb(0, 0, 0)\n",
    "color1 = rgb(132, 138, 147)\n",
    "color2 = rgb(34, 157, 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line(x, y, color, name=None):\n",
    "  return go.Scatter(\n",
    "    name=name,\n",
    "      x = x,\n",
    "      y = y,\n",
    "      hoverinfo = 'text',\n",
    "    mode = 'lines',\n",
    "    marker={'color':str(color)},\n",
    "    line_width=3,\n",
    "      \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bgcolor = \"rgb(220,220,220)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layout = dict(\n",
    "  plot_bgcolor= bgcolor,\n",
    "    width=800,\n",
    "    height=1760,\n",
    " #      template = 'ggplot2',\n",
    "    xaxis1 = {'domain': [0, 1], 'anchor': 'x1', 'range': [-6, 6], 'ticks':'inside', 'tick0':-6, 'dtick':2 },\n",
    "    yaxis1 = {'domain': [0.8, 1], 'anchor': 'x1', 'range': [-.1, 1.1], 'ticks':'inside',  'tick0':0, 'dtick':.5},\n",
    "    xaxis2 = {'domain': [0, 1], 'anchor': 'free', 'position':.04, 'range': [-2.1, 2.1], 'ticks':'inside', 'showgrid':True },\n",
    "    yaxis2 = {'domain': [.04, .21], 'anchor': 'free', 'range': [-.2, 2], 'position':.00, 'showticklabels':False, 'ticks':''},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x= np.arange(-6,6,.01)\n",
    "sig = np.array([sigmoid(i) for i in x])\n",
    "l1 = line(x, sig, color1, name='sigmoid')\n",
    "l2 = line(x, sig*(1-sig), color2, name='sigmoid gradient')\n",
    "l3 = line(x, (x>0).astype(np.int), color0, name='step')\n",
    "fig = go.Figure(data=[l1, l2, l3], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tucker/Downloads/xor_hidden.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.offline.plot(fig, filename='/home/tucker/Downloads/xor_hidden.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import subplots\n",
    "import plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nThe orca executable is required to export figures as static images,\nbut it could not be found on the system path.\n\nSearched for executable 'orca' on the following path:\n    /home/tucker/.cargo/bin\n    /usr/local/sbin\n    /usr/local/bin\n    /usr/bin\n    /opt/cuda/bin\n    /usr/bin/site_perl\n    /usr/bin/vendor_perl\n    /usr/bin/core_perl\n    /home/tucker/bin\n    /home/tucker/.local/bin\n    /home/tucker/anaconda3/bin\n    /home/tucker/bin\n    /home/tucker/.local/bin\n    /home/tucker/anaconda3/bin\n    /opt/cuda/bin\n    /usr/bin/site_perl\n    /usr/bin/vendor_perl\n    /usr/bin/core_perl\n    /home/tucker/bin\n    /home/tucker/.local/bin\n    /home/tucker/anaconda3/bin\n\nIf you haven't installed orca yet, you can do so using conda as follows:\n\n    $ conda install -c plotly plotly-orca\n\nAlternatively, see other installation methods in the orca project README at\nhttps://github.com/plotly/orca\n\nAfter installation is complete, no further configuration should be needed.\n\nIf you have installed orca, then for some reason plotly.py was unable to\nlocate it. In this case, set the `plotly.io.orca.config.executable`\nproperty to the full path of your orca executable. For example:\n\n    >>> plotly.io.orca.config.executable = '/path/to/orca'\n\nAfter updating this executable property, try the export operation again.\nIf it is successful then you may want to save this configuration so that it\nwill be applied automatically in future sessions. You can do this as follows:\n\n    >>> plotly.io.orca.config.save()\n\nIf you're still having trouble, feel free to ask for help on the forums at\nhttps://community.plot.ly/c/api/python\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-30a32d7e1b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../step_sigmoid_grad.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/plotly/basedatatypes.py\u001b[0m in \u001b[0;36mwrite_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2822\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;31m# Static helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/plotly/io/_orca.py\u001b[0m in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     \u001b[0;31m# -------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m     \u001b[0;31m# Do this first so we don't create a file if image conversion fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m     img_data = to_image(\n\u001b[0m\u001b[1;32m   1767\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m     )\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/plotly/io/_orca.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[0;31m# Make sure orca sever is running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m     \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m     \u001b[0mensure_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;31m# Handle defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/plotly/io/_orca.py\u001b[0m in \u001b[0;36mensure_server\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;31m# Validate orca executable only if server_url is not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"unvalidated\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mvalidate_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# Acquire lock to make sure that we keep the properties of orca_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# consistent across threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/plotly/io/_orca.py\u001b[0m in \u001b[0;36mvalidate_executable\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecutable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \"\"\"\n\u001b[1;32m   1078\u001b[0m \u001b[0mThe\u001b[0m \u001b[0morca\u001b[0m \u001b[0mexecutable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrequired\u001b[0m \u001b[0mto\u001b[0m \u001b[0mexport\u001b[0m \u001b[0mfigures\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatic\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nThe orca executable is required to export figures as static images,\nbut it could not be found on the system path.\n\nSearched for executable 'orca' on the following path:\n    /home/tucker/.cargo/bin\n    /usr/local/sbin\n    /usr/local/bin\n    /usr/bin\n    /opt/cuda/bin\n    /usr/bin/site_perl\n    /usr/bin/vendor_perl\n    /usr/bin/core_perl\n    /home/tucker/bin\n    /home/tucker/.local/bin\n    /home/tucker/anaconda3/bin\n    /home/tucker/bin\n    /home/tucker/.local/bin\n    /home/tucker/anaconda3/bin\n    /opt/cuda/bin\n    /usr/bin/site_perl\n    /usr/bin/vendor_perl\n    /usr/bin/core_perl\n    /home/tucker/bin\n    /home/tucker/.local/bin\n    /home/tucker/anaconda3/bin\n\nIf you haven't installed orca yet, you can do so using conda as follows:\n\n    $ conda install -c plotly plotly-orca\n\nAlternatively, see other installation methods in the orca project README at\nhttps://github.com/plotly/orca\n\nAfter installation is complete, no further configuration should be needed.\n\nIf you have installed orca, then for some reason plotly.py was unable to\nlocate it. In this case, set the `plotly.io.orca.config.executable`\nproperty to the full path of your orca executable. For example:\n\n    >>> plotly.io.orca.config.executable = '/path/to/orca'\n\nAfter updating this executable property, try the export operation again.\nIf it is successful then you may want to save this configuration so that it\nwill be applied automatically in future sessions. You can do this as follows:\n\n    >>> plotly.io.orca.config.save()\n\nIf you're still having trouble, feel free to ask for help on the forums at\nhttps://community.plot.ly/c/api/python\n"
     ]
    }
   ],
   "source": [
    "fig.write_image('../step_sigmoid_grad.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "ErrorAnalysis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
