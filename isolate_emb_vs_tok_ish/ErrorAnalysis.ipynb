{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from experiment import Net, InitDataset, CreateModel, LoadCheckpoint, RunOne\n",
    "from yaml import safe_load\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#good chars: dfb53a6b2\n",
    "#perfect emb: a2bb6c3c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exphash = 'aa0d32e62'\n",
    "exp = 'results/%s' % exphash\n",
    "with open(exp) as f:\n",
    "  h = safe_load(f.read())\n",
    "h = h['exp_info']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h['add_random_count'] = 0\n",
    "h['space_frequency']=0\n",
    "h['add_next_words']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = InitDataset(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:50:57] loading configuration file bert_distil_uncased/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:50:57] loading weights file bert_distil_uncased/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/aa0d32e62'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7253bb1cbb6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#h['hyperparameters']['manual_attention'] = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hyperparameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mLoadCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mexphash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sabbatical/predict_bert_embeddings/char_lm/isolate_emb_vs_tok_ish/experiment.py\u001b[0m in \u001b[0;36mLoadCheckpoint\u001b[0;34m(model, checkpoint_file)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoadCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded model: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/aa0d32e62'"
     ]
    }
   ],
   "source": [
    "#h['hyperparameters']['manual_attention'] = True\n",
    "model = CreateModel(h['hyperparameters'])\n",
    "LoadCheckpoint(model, 'models/%s'%exphash)\n",
    "\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h['dataset_split'][1] = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(ds[1], batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 4, 11, 46, 47]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label_ids': tensor([ 2000,  3058,  2045,  2003,  2053,  2338,  1997, 15111, 10505, 11262,\n",
       "          2626,  1037,  2048,  5717,  2337,  2028,  3157,  2698,  2416,  3661,\n",
       "          2000,  2720,  1996,  2613, 25323,  1997,  1996,  3259,  2003,  2008,\n",
       "          2017,  2064,  1056,  2275,  1996,  2338,  1997, 15111, 10505,  2091,\n",
       "          5973,  2138,  2009,  2003,  7214,  1998,  2097,  2196,  3113,  1996,\n",
       "          5918,  1997,  1996,  6900,  7905,  8780,  6483, 14387,  2096,  2169,\n",
       "          1997,  1996, 17289,  9339,  4216,  2024,  4089,  2310,  3089, 22749,\n",
       "          3468,  4401,  2228,  2009,  5875,  2000,  3602,  2008,  1996, 17627,\n",
       "          2277,  3075,  2515,  5383,  4809,  1997,  2169,  2021,  2027,  2024,\n",
       "          2025,  2112,  1997,  2151, 17627,  8882]),\n",
       " 'token_ids': tensor([ 2000,  3058,  2045,  2003,   103,  2338,  1997, 15111, 10505, 11262,\n",
       "          2626,   103,  2048,  5717,  2337,  2028,  3157,  2698,  2416,  3661,\n",
       "          2000,  2720,  1996,  2613, 25323,  1997,  1996,  3259,  2003,  2008,\n",
       "          2017,  2064,  1056,  2275,  1996,  2338,  1997, 15111, 10505,  2091,\n",
       "          5973,  2138,  2009,  2003,  7214,  1998,   103,   103,  3113,  1996,\n",
       "          5918,  1997,  1996,  6900,  7905,  8780,  6483, 14387,  2096,  2169,\n",
       "          1997,  1996, 17289,  9339,  4216,  2024,  4089,  2310,  3089, 22749,\n",
       "          3468,  4401,  2228,  2009,  5875,  2000,  3602,  2008,  1996, 17627,\n",
       "          2277,  3075,  2515,  5383,  4809,  1997,  2169,  2021,  2027,  2024,\n",
       "          2025,  2112,  1997,  2151, 17627,  8882]),\n",
       " 'end_of_word_index': tensor([ 2,  4,  5,  2,  3,  4,  2,  6,  9,  8,  5,  3,  3,  4,  8,  3,  4,  5,\n",
       "          3,  6,  2,  2,  3,  4, 11,  2,  3,  5,  2,  4,  3,  3,  1,  3,  3,  4,\n",
       "          2,  6,  9,  4,  8,  7,  2,  2,  9,  3,  3,  3,  4,  3, 12,  2,  3,  4,\n",
       "          4,  2,  4,  3,  5,  4,  2,  3, 14,  6,  7,  3,  6,  2,  2,  3,  3,  7,\n",
       "          5,  2, 11,  2,  4,  4,  3,  3,  6,  7,  4,  7,  6,  2,  4,  3,  4,  3,\n",
       "          3,  4,  2,  3,  3, 10]),\n",
       " 'input_ids': tensor([[63, 58,  1,  ..., 62,  1, 32],\n",
       "         [47, 44, 63,  ..., 56, 33,  1],\n",
       "         [63, 51, 48,  ..., 58, 54,  1],\n",
       "         ...,\n",
       "         [44, 57, 68,  ..., 55, 64, 56],\n",
       "         [55, 47, 62,  ..., 32, 59, 44],\n",
       "         [46, 64, 61,  ..., 33,  1, 32]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_to_idx_map = torch.load('../data/char_to_idx_map2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Torch2Py = lambda x: x.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_to_char_map= {v:k for k,v in char_to_idx_map.items()}\n",
    "idx_to_tok_map = {}\n",
    "with open('../data/bert-base-uncased-vocab.txt') as f:\n",
    "  for i, l in enumerate(f.readlines()):\n",
    "    idx_to_tok_map[i] = l.strip()\n",
    "def GetChars(encoded):\n",
    "  s = \"\"\n",
    "  for c in encoded:\n",
    "    s+= idx_to_char_map[c]\n",
    "  return s.replace('\\x00','_')\n",
    "def GetToks(encoded):\n",
    "  s = \"\"\n",
    "  for c in encoded:\n",
    "    s+= \" \" + idx_to_tok_map[c]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 1, 14, 19, 25, 28, 43, 65, 67, 80, 85]),)\n"
     ]
    }
   ],
   "source": [
    "d = ds[1][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' furthermore [MASK] movie titled awesome i fuck ##in shot that featuring a live performance [MASK] october nine two zero [MASK] four at madison square garden [MASK] scheduled for [MASK] on march three one two zero zero six directed by nathan ##ial h rn [MASK] ##ow r it was taped by five zero fans in the crowd who were given hi eight video cameras the film [MASK] first [MASK] on january six two zero zero six to the fans that shot [MASK] footage it premiered at [MASK] sundance film festival in january and the dvd of the'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(d['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furthermore [m] mo\n",
      "[m] movie titled a\n",
      "movie titled aweso\n",
      "titled awesome i i\n",
      "awesome i fuckin s\n",
      "i fuckin shot that\n",
      "fuckin shot that f\n",
      "in shot that featu\n",
      "shot that featurin\n",
      "that featuring a l\n",
      "featuring a live p\n",
      "a live performance\n",
      "live performance [\n",
      "performance [m] oc\n",
      "[m] october nine t\n",
      "october nine two z\n",
      "nine two zero [m] \n",
      "two zero [m] four \n",
      "zero [m] four at m\n",
      "[m] four at madiso\n",
      "four at madison sq\n",
      "at madison square \n",
      "madison square gar\n",
      "square garden [m] \n",
      "garden [m] schedul\n",
      "[m] scheduled for \n",
      "scheduled for [m] \n",
      "for [m] on march t\n",
      "[m] on march three\n",
      "on march three one\n",
      "march three one tw\n",
      "three one two zero\n",
      "one two zero zero \n",
      "two zero zero six \n",
      "zero zero six dire\n",
      "zero six directed \n",
      "six directed by ni\n",
      "directed by nathai\n",
      "by nathanial h rn_\n",
      "nathanial h rn [m]\n",
      "ial h rn [m]ow____\n",
      "h rn [m]ow r______\n",
      "rn [m]ow r it_____\n",
      "[m]ow r it was____\n",
      "ow r it was taped_\n",
      "r it was taped by_\n",
      "it was taped by fi\n",
      "was taped by five \n",
      "taped by five zero\n",
      "by five zero fans \n",
      "five zero fans in \n",
      "zero fans in the c\n",
      "fans in the crowd \n",
      "in the crowd who w\n",
      "the crowd who were\n",
      "crowd who were giv\n",
      "who were given hi \n",
      "were given hi eigh\n",
      "given hi eight vid\n",
      "hi eight video cam\n",
      "eight video camera\n",
      "video cameras the \n",
      "cameras the film [\n",
      "the film [m] first\n"
     ]
    }
   ],
   "source": [
    "for i in range(64):\n",
    "  print(GetChars(Torch2Py(d['input_ids'][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "w = '[cls] guitars [m] some ot [m] [pad] [pad]'\n",
    "thing = []\n",
    "for c in w :\n",
    "  thing.append(char_to_idx_map[c])\n",
    "print(len(thing))\n",
    "inp = torch.tensor(thing).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32, 46, 55, 62, 33,  1, 50, 64, 52, 63, 44, 61, 62,  1, 32, 56, 33,  1,\n",
       "         62, 58, 56, 48,  1, 58, 63,  1, 32, 56, 33,  1, 32, 59, 44, 47, 33,  1,\n",
       "         32, 59, 44, 47, 33]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp_ids = ds[1][0]['input_ids'].unsqueeze(0)\n",
    "label_ids = ds[1][0]['label_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_ids = ds[1][0]['token_ids'].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[m]_______________\n",
      "ing_______________\n",
      "a_________________\n",
      "quarter___________\n",
      "is________________\n",
      "the_______________\n",
      "male______________\n",
      "attacking_________\n",
      "the_______________\n",
      "female____________\n",
      "and_______________\n",
      "the_______________\n",
      "remaining_________\n",
      "[m]_______________\n",
      "[m]_______________\n",
      "females___________\n",
      "attacking_________\n",
      "their_____________\n",
      "male______________\n",
      "partner___________\n",
      "[m]_______________\n",
      "how_______________\n",
      "many______________\n",
      "instances_________\n",
      "of________________\n",
      "domestic__________\n",
      "[m]_______________\n",
      "actually__________\n",
      "involve___________\n",
      "male______________\n",
      "victims___________\n",
      "is________________\n",
      "difficult_________\n",
      "[m]_______________\n",
      "domestic__________\n",
      "violence__________\n",
      "victims___________\n",
      "may_______________\n",
      "be________________\n",
      "reluctant_________\n",
      "to________________\n",
      "get_______________\n",
      "help______________\n",
      "for_______________\n",
      "a_________________\n",
      "number____________\n",
      "of________________\n",
      "[m]_______________\n",
      "see_______________\n",
      "this______________\n",
      "article___________\n",
      "article___________\n",
      "checked___________\n",
      "august____________\n",
      "eight_____________\n",
      "two_______________\n",
      "zero______________\n",
      "zero______________\n",
      "four______________\n",
      "a_________________\n",
      "man_______________\n",
      "who_______________\n",
      "calls_____________\n",
      "for_______________\n",
      "help______________\n",
      "may_______________\n",
      "even______________\n",
      "[m]_______________\n",
      "[m]_______________\n",
      "arrested__________\n",
      "as________________\n",
      "[m]_______________\n",
      "[m]_______________\n",
      "pet_______________\n",
      "rator_____________\n",
      "even______________\n",
      "though____________\n",
      "he________________\n",
      "was_______________\n",
      "the_______________\n",
      "victim____________\n",
      "the_______________\n",
      "general___________\n",
      "consensus_________\n",
      "seems_____________\n",
      "[m]_______________\n",
      "[m]_______________\n",
      "that______________\n",
      "[m]_______________\n",
      "on________________\n",
      "female____________\n",
      "domestic__________\n",
      "[m]_______________\n",
      "is________________\n",
      "more______________\n",
      "likely____________\n"
     ]
    }
   ],
   "source": [
    "for i in inp_ids[0]:\n",
    "  print(GetChars(Torch2Py(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' brawl ##ing a quarter is the male attacking the female and the remaining quarter being females attacking their male partner determining how many instances of domestic violence actually involve male victims is difficult male domestic violence victims may be reluctant to get help for a number of reasons see this article article checked august eight two zero zero four a man who calls for help may even risk being arrested as the per ##pet ##rator even though he was the victim the general consensus seems to be that male on female domestic violence is more likely'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 18])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_ids.shape #chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = model(inp_ids.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moutwords = output.argmax(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' . \" a quarter is the male attacking the female and the female male male males attacking their male partner . how many instances of sexual violence actually multiple male victims are difficult romance domestic violence victims may be interested to please help please a number of please see from article article contents august eight two zero zero four a man and calls for help may , child victims arrested as male male pet ##rator even though he was the victim the general consensus seems . conclusion that assault on female domestic violence is more likely'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives a garbage output\n",
    "GetToks(Torch2Py(moutwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we check the inputs that are put into  the distilbert model (assumption maybe wrong) ---hold on\n",
    "inp_embs = model.bert.distilbert.inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [CLS] ##ing a quarter is the male 910 the ##dna and the remaining [CLS] [CLS] ##dna ##rdon their male partner [CLS] how 690 ##rdon of domestic [CLS] 930 ##omba male victims is 670 [CLS] domestic ##omba victims may 243 ##hner to get help for a number of [CLS] see this article article checked 740 820 two zero zero 276 a 243 who calls for help may even [CLS] [CLS] ##anor as [CLS] [CLS] pet ##rator even though 670 was the ##anor the general consensus ##anor [CLS] [CLS] that [CLS] on ##dna domestic [CLS] 930 more likely'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with and without position embeddings we for the most part get the word back checking this way.\n",
    "inp_embs = model.embedded_chars\n",
    "#inp_embs = model.embedded_chars_no_pos\n",
    "word = (inp_embs@model.bert.distilbert.embeddings.word_embeddings.weight.data.T).argmax(-1)[0]\n",
    "GetToks(Torch2Py(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " one the one quarter is the male attacking the victim and the remaining quarter being female male the male partner . how many instances of domestic violence actually involve male victims is difficult male domestic violence victims may be reluctant to get help for a variety of reasons see this article article checked august eight two zero zero four a man who calls for help may even consider being identified as the per ##pet ##rator even though he was the victim the general consensus seems to be that focusing on female domestic violence is more likely\n",
      "AA\n",
      " romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance romance\n"
     ]
    }
   ],
   "source": [
    "# giving the model the token ids directly we get good results as expected. this is vanilla bert.\n",
    "outputstok = model.bert(input_ids=token_ids)\n",
    "tokoutwords = outputstok[0].argmax(-1)[0]\n",
    "print(GetToks(Torch2Py(tokoutwords)))\n",
    "\n",
    "# so if we run this and grab the input embeddings just before they go into the transformer\n",
    "inp_emb_tok_pos = model.bert.distilbert.inputs_embeds\n",
    "inp_emb_tok_no_pos = model.bert.distilbert.inputs_embeds_no_pos\n",
    "outputstok = model.bert(inputs_embeds=inp_emb_tok_no_pos)\n",
    "tokoutwords = outputstok[0].argmax(-1)[0]\n",
    "print(\"AA\")\n",
    "print(GetToks(Torch2Py(tokoutwords)))\n",
    "# no pos gives garbage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' brawl ##ing [CLS] quarter is the male ##rdon the [CLS] and [CLS] remaining quarter being females [CLS] [CLS] male partner [CLS] how many ##rdon of domestic ##anor 670 [CLS] male ##anor is 670 male domestic ##anor ##anor may be ##rdon to 930 help for a number of reasons see this article article ##rdon 740 840 two zero zero 276 a man who calls for help may even [CLS] being [CLS] as the per [CLS] ##rator even though 670 was the ##dna the general consensus ##anor to be that [CLS] on 690 domestic ##anor is [CLS] likely'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the result of looking at the input embeddings when compared directly with the emb matrix shows similar to the char predembs\n",
    "out = model.bert.distilbert.inputs_embeds_no_pos\n",
    "word = (out@model.bert.distilbert.embeddings.word_embeddings.weight.data.T).argmax(-1)[0]\n",
    "GetToks(Torch2Py(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-dc8d20b402bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minp_embs_no_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedded_chars_no_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0minp_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedded_chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutputsemb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp_embs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcharoutwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputsemb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetToks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorch2Py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharoutwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_emb' is not defined"
     ]
    }
   ],
   "source": [
    "# if we input the embeddings from the char embedder with and without pos what happens?\n",
    "# without pos its garbage just ------- across the board.\n",
    "# with pos its basically the same but with a single . instead of all - - -\n",
    "\n",
    "# BUG: adding the position embeddings from the vanilla bert model gives the right output. so im just adding the pos embeddings wrong in my experiments.\n",
    "# i wonder if theyre getting initialized to new values when the model initializes?!\n",
    "output = model(inp_ids.cuda())\n",
    "\n",
    "inp_embs_no_pos = model.embedded_chars_no_pos\n",
    "inp_embs = model.embedded_chars\n",
    "outputsemb = model.bert(inputs_embeds=inp_embs+pos_emb)\n",
    "charoutwords = outputsemb[0].argmax(-1)[0]\n",
    "print(GetToks(Torch2Py(charoutwords)))\n",
    "print()\n",
    "charoutwords = output[0].argmax(-1)\n",
    "print(GetToks(Torch2Py(charoutwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so the position embeddings are correct...\n",
    "(model.pos_emb_inst == model.bert.distilbert.embeddings.position_embeddings_inst).float().mean()\n",
    "# FAAAACK I FORGOT THE LAYER NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
       "           6.8312e-04,  1.5441e-02],\n",
       "         [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
       "           2.9753e-02, -5.3247e-03],\n",
       "         [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
       "           1.8741e-02, -7.3140e-03],\n",
       "         ...,\n",
       "         [ 1.0013e-03, -1.8523e-03,  6.4189e-03,  ..., -7.2757e-03,\n",
       "           1.7281e-03,  2.8407e-03],\n",
       "         [-3.6395e-03,  8.9983e-03,  1.3761e-02,  ..., -3.5218e-03,\n",
       "           3.5612e-04,  7.9451e-03],\n",
       "         [-1.8650e-02,  7.7867e-03,  1.8862e-02,  ..., -1.9309e-02,\n",
       "           2.0698e-02,  9.4798e-03]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
       "          6.8312e-04,  1.5441e-02],\n",
       "        [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
       "          2.9753e-02, -5.3247e-03],\n",
       "        [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
       "          1.8741e-02, -7.3140e-03],\n",
       "        ...,\n",
       "        [ 1.7418e-02,  3.4903e-03, -9.5621e-03,  ...,  2.9599e-03,\n",
       "          4.3435e-04, -2.6949e-02],\n",
       "        [ 2.1687e-02, -6.0216e-03,  1.4736e-02,  ..., -5.6118e-03,\n",
       "         -1.2590e-02, -2.8085e-02],\n",
       "        [ 2.6413e-03, -2.3298e-02,  5.4922e-03,  ...,  1.7537e-02,\n",
       "          2.7550e-02, -7.7656e-02]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.position_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
       "          6.8312e-04,  1.5441e-02],\n",
       "        [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
       "          2.9753e-02, -5.3247e-03],\n",
       "        [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
       "          1.8741e-02, -7.3140e-03],\n",
       "        ...,\n",
       "        [ 1.7418e-02,  3.4903e-03, -9.5621e-03,  ...,  2.9599e-03,\n",
       "          4.3435e-04, -2.6949e-02],\n",
       "        [ 2.1687e-02, -6.0216e-03,  1.4736e-02,  ..., -5.6118e-03,\n",
       "         -1.2590e-02, -2.8085e-02],\n",
       "        [ 2.6413e-03, -2.3298e-02,  5.4922e-03,  ...,  1.7537e-02,\n",
       "          2.7550e-02, -7.7656e-02]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.distilbert.embeddings.position_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "         72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "         90, 91, 92, 93, 94, 95]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = ((out[0, :8].unsqueeze(1)-model.bert.distilbert.embeddings.word_embeddings.weight.data)**2).mean(-1).argmin(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' purpose article two [MASK] of scientific investigation and'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetToks(Torch2Py(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_emb = inp_emb_tok_pos - inp_emb_tok_no_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "ErrorAnalysis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
