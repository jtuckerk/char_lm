{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_emb = bert.embeddings.word_embeddings.weight.data\n",
    "bert_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(bert_emb, \"distilbert_embedding_matrix.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bert_vocab = []\n",
    "with open('bert-base-uncased-vocab.txt') as f:\n",
    "  for l in f.readlines():\n",
    "    bert_vocab.append(l.strip())\n",
    "\n",
    "assert len(bert_vocab) == len(bert_emb)\n",
    "longest_word_in_bert_vocab = max([len(w) for w in bert_vocab])\n",
    "word_length = 12\n",
    "def GetCharEncoding(word):\n",
    "  enc = [0]*word_length\n",
    "  for i, c in enumerate(word):\n",
    "    if i>= word_length:\n",
    "      break\n",
    "    enc[i] = char_to_idx_map.get(c,0)\n",
    "  return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb = torch.load('../../../character_convolution/data/torch_imdb.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_to_idx_map =  imdb['char_to_idx_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(char_to_idx_map, 'char_to_idx_map.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = ['\\x00', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.',\n",
    "         '/', ':', ';', '<', '=', '>', '?', '@', '\\\\', '^', '_', '`', '{', '|', '}', '~', '[',  ']',\n",
    "         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "         'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_to_idx_map2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 56, 33)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx_map2['['], char_to_idx_map2['m'], char_to_idx_map2[']']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, c in enumerate(chars):\n",
    "  char_to_idx_map2[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(char_to_idx_map2, 'char_to_idx_map2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_encodings = []\n",
    "for word in bert_vocab:\n",
    "  encoding = GetCharEncoding(word.lower())\n",
    "  word_encodings.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 55, 40, 43, 36, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "repeat_map = defaultdict(int)\n",
    "for w in word_encodings:\n",
    "  repeat_map[tuple(w)]+=1\n",
    "valid_list = [1 if x<=1 else 0 for x in repeat_map.values()]\n",
    "word_char_encoding = torch.tensor(word_encodings, dtype=torch.int64, device='cuda')\n",
    "valid_list = torch.tensor(valid_list, dtype=torch.int64, device='cuda')\n",
    "word_char_encoding = word_char_encoding[torch.where(valid_list==1)]\n",
    "embeddings = bert_emb[torch.where(valid_list==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['word_char_encoding'] = word_char_encoding.cuda()\n",
    "data['embeddings'] = embeddings.cuda()\n",
    "data['char_to_idx_map'] = char_to_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(data, 'preproc_distilbert_vocab_emb.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 way comparison -- load preprocessed data directly into GPU\n",
    "# load embedding dict and word text dict into cpu and run multiprocessing to load each word into GPU during training\n",
    "   # more flexible and simpler code (can be a part of the experiment at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VocabDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"Dataset with features: token characters and labels: token index or embedding\n",
    "     provides a way to add random words after the token of interest and random misspellings.\n",
    "  \"\"\"     \n",
    "  def __init__(self, vocab_file, char_to_idx_file, embedding_file, word_length, shuffle=False, normalize=False,\n",
    "               misspelling_rate=None, misspelling_transforms=None, misspelling_type=None,\n",
    "               add_next_word=False, add_random_count=0, space_freq=1.0, device='cpu'):\n",
    "    self.data = self._preprocess(vocab_file, char_to_idx_file, embedding_file, word_length)\n",
    "    self.keys=['word_char_encoding', 'embeddings']\n",
    "    if normalize:\n",
    "      self.data['embeddings'] = self.data['embeddings']/self.data['embeddings'].norm(dim=-1, p=2).unsqueeze(-1)\n",
    "    self.misspelling_rate =misspelling_rate\n",
    "    self.misspelling_transforms=misspelling_transforms\n",
    "    self.misspelling_type=misspelling_type\n",
    "    self.add_next_word=add_next_word\n",
    "    self.device=device\n",
    "    self.add_random_count = add_random_count\n",
    "    self.space_freq = space_freq\n",
    "    self.embedding_matrix = self.data['embeddings']\n",
    "    if shuffle:\n",
    "      r = torch.randperm(self.nelement())\n",
    "      for k in self.keys:\n",
    "        self.data[k][r] = self.data[k]\n",
    "\n",
    "    self.data['word_indices'] = torch.arange(len(self.data['embeddings']))\n",
    "    self.char_to_idx_map = self.data['char_to_idx_map']\n",
    "\n",
    "    chars_for_insertion = [\n",
    "         'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "         'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "    self.insertion_lookup = torch.tensor([self.char_to_idx_map[c] for c in chars_for_insertion])\n",
    "    self.transform_options = [self.add_letter,\n",
    "                              self.substitute_letter,\n",
    "                              self.transpose_letters,\n",
    "                              self.delete_letter,\n",
    "                              self.repeat_letter]\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data[self.keys[0]])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if self.misspelling_rate == \"not set\":\n",
    "      raise \"Must set mispelling rate (or explicitly set to None) as a hyperparameter\"\n",
    "    char_encoded_input = self.data['word_char_encoding'][idx]\n",
    "\n",
    "    item = {\n",
    "      'target_embeddings': self.data['embeddings'][idx],\n",
    "      'labels': self.data['word_indices'][idx],\n",
    "    }\n",
    "\n",
    "    if self.misspelling_rate and torch.rand((1,)) <= self.misspelling_rate:\n",
    "      char_encoded_input = self.apply_misspelling(char_encoded_input)\n",
    "\n",
    "    first_end_ind = None\n",
    "    for i in range(self.add_random_count):\n",
    "      if torch.rand(size=())<.8: # skip 1/5 of the time\n",
    "        char_encoded_input, end_indx=self.add_random_next_word(char_encoded_input)\n",
    "        if not first_end_ind:\n",
    "          first_end_ind = end_indx\n",
    "          #item['end_of_word_index'] = first_end_ind\n",
    "    item['features'] = char_encoded_input\n",
    "    \n",
    "    return item\n",
    "  \n",
    "  def GetCharEncoding(word, char_to_idx_map, word_length):\n",
    "    enc = [0]*word_length\n",
    "    for i, c in enumerate(word):\n",
    "      if i>= word_length:\n",
    "        break\n",
    "      enc[i] = char_to_idx_map.get(c,0)\n",
    "    return enc\n",
    "\n",
    "  def _preprocess(self, vocab_file, char_to_idx_file, embedding_file, word_length):\n",
    "    bert_vocab = []\n",
    "    with open(vocab_file) as f:\n",
    "      for l in f.readlines():\n",
    "        bert_vocab.append(l.strip())\n",
    "\n",
    "    bert_emb = torch.load(embedding_file)\n",
    "    assert len(bert_vocab) == len(bert_emb)\n",
    "    longest_word_in_bert_vocab = max([len(w) for w in bert_vocab])\n",
    "\n",
    "    char_to_idx_map = torch.load(char_to_idx_file)\n",
    "    \n",
    "    word_encodings = []\n",
    "    for word in bert_vocab:\n",
    "      encoding = self.GetCharEncoding(word.lower(), char_to_idx_map, word_length)\n",
    "      word_encodings.append(encoding)\n",
    "      \n",
    "    repeat_map = defaultdict(int)\n",
    "    for w in word_encodings:\n",
    "      repeat_map[tuple(w)]+=1\n",
    "    valid_list = [1 if x<=1 else 0 for x in repeat_map.values()]\n",
    "    word_char_encoding = torch.tensor(word_encodings, dtype=torch.int64)\n",
    "    valid_list = torch.tensor(valid_list, dtype=torch.int64)\n",
    "    word_char_encoding = word_char_encoding[torch.where(valid_list==1)]\n",
    "\n",
    "    embeddings = bert_emb[torch.where(valid_list==1)].to(dev)\n",
    "    return {\"word_char_encoding\":word_char_encoding,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"char_to_idx_map\":char_to_idx_map}\n",
    "\n",
    "  def random_location(self, word_size):\n",
    "    return torch.randint(0, max(word_size, 0), size=())\n",
    "\n",
    "  def random_letter(self,):\n",
    "    return self.insertion_lookup[torch.randint(0, self.insertion_lookup.shape[0], size=())]\n",
    "\n",
    "  def word_len(self, word):\n",
    "    return (word!=0).sum()\n",
    "  \n",
    "  def add_letter(self, word):\n",
    "    word = word.clone()\n",
    "    n = self.random_location(min(word.shape[0], self.word_len(word)+1))\n",
    "    p2 = word[n:].clone()\n",
    "    rl = self.random_letter()\n",
    "    word[n+1:] = p2[:-1] # this will remove the last letter of some words (very few. not concerned for now)\n",
    "    word[n] = rl  \n",
    "    return word\n",
    "\n",
    "  def repeat_letter(self, word):\n",
    "    word = word.clone()\n",
    "    n = self.random_location(self.word_len(word))\n",
    "    p2 = word[n:].clone()\n",
    "    word[n+1:] = p2[:-1] # this will remove the last letter of some words (very few. not concerned for now)\n",
    "    return word\n",
    "  \n",
    "  def substitute_letter(self, word):\n",
    "    word = word.clone()\n",
    "    n = self.random_location(self.word_len(word))\n",
    "    rl = self.random_letter()\n",
    "    word[n] = rl\n",
    "    return word\n",
    "\n",
    "  def delete_letter(self, word):\n",
    "    word = word.clone()\n",
    "    n = self.random_location(self.word_len(word))\n",
    "    word[n:-1] = word[n+1:].clone()\n",
    "    word[-1] = 0\n",
    "    return word\n",
    "\n",
    "  def transpose_letters(self, word):\n",
    "    word = word.clone()\n",
    "    n = self.random_location(self.word_len(word)-1)\n",
    "    t = word[n].clone()\n",
    "    word[n] = word[n+1]\n",
    "    word[n+1] = t\n",
    "    return word\n",
    "\n",
    "  def AlterWord(self, word, transforms, misspelling_type=None):\n",
    "    if self.word_len(word) <= 2:\n",
    "      return word\n",
    "\n",
    "    if misspelling_type == \"add\":\n",
    "      change_fn = self.add_letter\n",
    "    if misspelling_type == \"repeat\":\n",
    "      change_fn = self.repeat_letter      \n",
    "    if misspelling_type == \"substitute\":\n",
    "      change_fn = self.substitute_letter\n",
    "    if misspelling_type == \"delete\":\n",
    "      change_fn = self.delete_letter\n",
    "    if misspelling_type == \"transpose\":\n",
    "      change_fn = self.transpose_letters\n",
    "\n",
    "    for i in range(transforms):\n",
    "      if not misspelling_type:\n",
    "        change_fn = self.transform_options[torch.randint(0, len(self.transform_options), size=())]\n",
    "      word = change_fn(word)\n",
    "    return word\n",
    "\n",
    "  def apply_misspelling(self, char_encoded_input):\n",
    "    return self.AlterWord(char_encoded_input,\n",
    "                          transforms=self.misspelling_transforms,\n",
    "                          misspelling_type=self.misspelling_type\n",
    "    )\n",
    "  \n",
    "  def add_random_next_word(self, char_encoded_input):\n",
    "    # returns the word with a random word added at the end of the word\n",
    "    # and the index after the end of the word.\n",
    "    pads = torch.where(char_encoded_input==0)\n",
    "    if pads[0].nelement() == 0:\n",
    "      idx = torch.LongTensor(char_encoded_input.size())[0] -1\n",
    "      return char_encoded_input, idx\n",
    "    end = pads[0][0]\n",
    "    add_word = torch.randint(0, len(self.data['embeddings']), ())\n",
    "    char_encoded_input = char_encoded_input.clone()\n",
    "    # add a space 1/2 the time\n",
    "    if torch.rand(size=())>=self.space_freq:\n",
    "      char_encoded_input[end:] = self.data['word_char_encoding'][add_word][:len(char_encoded_input)-end]\n",
    "    else:\n",
    "      char_encoded_input[end] = self.data['char_to_idx_map'][' '] # space char\n",
    "      char_encoded_input[end+1:] = self.data['word_char_encoding'][add_word][:len(char_encoded_input)-end-1]\n",
    "    return char_encoded_input, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 s ± 14.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "vocab_file = 'bert-base-uncased-vocab.txt'\n",
    "char_to_idx_file = 'char_to_idx_map.pt'\n",
    "embedding_file = 'distilbert_embedding_matrix.pt'\n",
    "bs = 128\n",
    "vds = VocabDataset(vocab_file, char_to_idx_file, embedding_file, add_random_count=2)\n",
    "train_loader = DataLoader(vds, batch_size=bs, shuffle=True, num_workers=10)\n",
    "for i in range(1):\n",
    "  for d in train_loader:\n",
    "    a = d['target_embeddings'].cuda()\n",
    "    a+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preproc inline: 299 ms ± 4.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# add 2 random words: 9.66 s ± 37.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "#  all on GPU -- gonna add random words on cpu and see how that looks?\n",
    "\n",
    "# do everything on the cpu with 12 workers and then convert to gpu: 957 ms ± 26.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# 2 workers: 1.88 s ± 29 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# 1 worker: 3.41 s ± 29.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# without load preproc load: 188 ms ± 2.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# with load preproc: 245 ms ± 1.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "SingleWordDataProc.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
